[
  {
    "objectID": "user_guide/index.html",
    "href": "user_guide/index.html",
    "title": "User Guide",
    "section": "",
    "text": "This is the user guide for neuralyzer",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "user_guide/data_loading/JSON_loading.html",
    "href": "user_guide/data_loading/JSON_loading.html",
    "title": "JSON_loading",
    "section": "",
    "text": "Digital Event Series\nDigital event series are data represented by an ordered sequence of timestamps. Examples include spike timestamps from extracellular recordings or behavioral events (e.g. go cue, reward given).\n\n\nDigital Interval Series\n\n16 bit binary representation\n{\n  \"filepath\": \"ttl.bin\",\n  \"data_type\": \"digital_interval\",\n  \"name\": \"laser\",\n  \"format\": \"uint16\",\n  \"channel\": 2, // REQUIRED, bit (0 based) for channel of interest\n  \"transition\": \"rising\", //optional, \n  \"clock\": \"master\", //optional, clock signal to assign to these events\n  \"header_size\": 0 //optional, number of bytes to skip at start of file\n}\n\n\n\n\n\n\n\n\n\n\nElement\nDescription\nRequired?\nType\nNotes\n\n\n\n\nfilepath\nPath to binary file, relative to JSON file.\nYes\nstring\n\n\n\ndata_type\n\nYes\nstring\n“digital_interval”\n\n\nname\nName of the data once it is loaded into Neuralyzer\nYes\nstring\n\n\n\nformat\n\nYes\nstring\n“uint16”\n\n\nchannel\nSpecifies which bit in binary representation should be extracted as digital interval\nNo\nnumber\nDefault is 0. Valid values range from 0-15\n\n\nTransition\n“rising” will count a TTL interval as one extending from low-to-high transitions to high-to-low transitions. “falling” will count a TTL interval as one extending from high-to-low to low-to-high transitions.\nNo\nstring\nDefault is “rising”. Valid values are “rising” or “falling”.\n\n\nclock\nClock signal to associate with this digital interval\nNo\nstring\nThe clock string must match the name of a loaded clock signal.\n\n\nheader_size\nThis many bytes will be skipped at the beginning of the file before reading the rest.\nNo\nnumber\nDefault is 0. Accepted values range from 0 to size of file in bytes.\n\n\n\n\n\n\n\n\nCSV\n\n{\n  \"filepath\": \"ttl.bin\",\n  \"data_type\": \"digital_interval\",\n  \"name\": \"laser\",\n  \"format\": \"csv\"\n\n}\n\n\n\n\n\n\n\n\n\n\nElement\nDescription\nRequired?\nType\nNotes\n\n\n\n\nfilepath\nPath to csv file, relative to JSON file.\nYes\nstring\n\n\n\ndata_type\n\nYes\nstring\n“digital_interval”\n\n\nname\nName of the data once it is loaded into Neuralyzer\nYes\nstring\n\n\n\nformat\n\nYes\nstring\n“csv”",
    "crumbs": [
      "Data Loading",
      "JSON_loading"
    ]
  },
  {
    "objectID": "user_guide/behaviors/tongue.html",
    "href": "user_guide/behaviors/tongue.html",
    "title": "Tongue Tracking",
    "section": "",
    "text": "The Tongue Tracking widget deals with operations related to the tongue.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "user_guide/behaviors/tongue.html#loading",
    "href": "user_guide/behaviors/tongue.html#loading",
    "title": "Tongue Tracking",
    "section": "Loading",
    "text": "Loading\nTongue masks can be loaded through the sparse HDF5 format or binary images (where white is part of the mask, black is not).\nJaw keypoint tracking can also be loaded through CSV format. The first column should indicate frame number, the next indicating \\(x\\) position and the next \\(y\\) position.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "user_guide/behaviors/tongue.html#grabcut-tool",
    "href": "user_guide/behaviors/tongue.html#grabcut-tool",
    "title": "Tongue Tracking",
    "section": "GrabCut Tool",
    "text": "GrabCut Tool\nDocumentation on the GrabCut tool can be found here.",
    "crumbs": [
      "Behavioral Modules",
      "Tongue Tracking"
    ]
  },
  {
    "objectID": "developer/contributing_code.html",
    "href": "developer/contributing_code.html",
    "title": "contributing_code",
    "section": "",
    "text": "Header files should use the hpp suffix, and source files should use the cpp suffix\nIncludes should follow the “Lakos” include order, that is\n\nThe prototype/interface header for this implementation (ie, the .h/.hh file that corresponds to this .cpp/.cc file).\nOther headers from the same project, as needed.\nHeaders from other non-standard, non-system libraries (for example, Qt, Eigen, etc).\nHeaders from other “almost-standard” libraries (for example, Boost)\nStandard C++ headers (for example, iostream, functional, etc.)\nStandard C headers (for example, cstdint, dirent.h, etc.)\n\nPrefer returning std::optional as a mechanism of error handling\nThis is a scientific computing library. Performance is critical. Helping the user to understand where errors have occurred is helpful, but keeping the program alive after an error is not critical. Functions should fail gracefully and provide informative error messages when they do. Logging should use spdlog.\nPrefer free functions as much as possible. Ideally, class member functions will be simple and pass member variables to free functions.\nPrefer standard library algorithms where possible\nPublic member functions and free function declarations should include doxygen comments above them. Private member function definitions should include doxygen comments above them.\nPrefer forward declarations in header files\nThis is a C++20 project. Prefer standard library algorithms and std::ranges where possible.",
    "crumbs": [
      "contributing_code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#design-guidelines",
    "href": "developer/contributing_code.html#design-guidelines",
    "title": "contributing_code",
    "section": "",
    "text": "Header files should use the hpp suffix, and source files should use the cpp suffix\nIncludes should follow the “Lakos” include order, that is\n\nThe prototype/interface header for this implementation (ie, the .h/.hh file that corresponds to this .cpp/.cc file).\nOther headers from the same project, as needed.\nHeaders from other non-standard, non-system libraries (for example, Qt, Eigen, etc).\nHeaders from other “almost-standard” libraries (for example, Boost)\nStandard C++ headers (for example, iostream, functional, etc.)\nStandard C headers (for example, cstdint, dirent.h, etc.)\n\nPrefer returning std::optional as a mechanism of error handling\nThis is a scientific computing library. Performance is critical. Helping the user to understand where errors have occurred is helpful, but keeping the program alive after an error is not critical. Functions should fail gracefully and provide informative error messages when they do. Logging should use spdlog.\nPrefer free functions as much as possible. Ideally, class member functions will be simple and pass member variables to free functions.\nPrefer standard library algorithms where possible\nPublic member functions and free function declarations should include doxygen comments above them. Private member function definitions should include doxygen comments above them.\nPrefer forward declarations in header files\nThis is a C++20 project. Prefer standard library algorithms and std::ranges where possible.",
    "crumbs": [
      "contributing_code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#testing",
    "href": "developer/contributing_code.html#testing",
    "title": "contributing_code",
    "section": "Testing",
    "text": "Testing",
    "crumbs": [
      "contributing_code"
    ]
  },
  {
    "objectID": "developer/contributing_code.html#further-reading-and-references",
    "href": "developer/contributing_code.html#further-reading-and-references",
    "title": "contributing_code",
    "section": "Further Reading and References",
    "text": "Further Reading and References\n\nTesting",
    "crumbs": [
      "contributing_code"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "developer/data_manager.html",
    "href": "developer/data_manager.html",
    "title": "Data Manager",
    "section": "",
    "text": "A Point represents a 2 dimensional (x, y) coordinate, which may vary in time. The PointData structure can hold multiple points per unit time.\n\n\n\nExamples of Point Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Line represents a 2 dimensional (x,y) collection of points, which may vary with time. The collection of points is ordered, meaning that each point is positioned relative to its neighbors. The LineData structure can hold multiple lines per unit time.\n\n\n\nExamples of Line Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA mask represents a 2 dimensional collection of points, which may vary with time. Compared to a Line, the points in a Mask are not ordered. These would correspond to something like a binary segmentation mask over an image. The MaskData structure can hold multiple masks per unit time.\n\n\n\nExamples of Mask Data\n\n\n\n\nBinary Semantic Segmentation Labels\n\n\n\n\n\n\n\n\n\n\n\n\nTensors are N-Dimensional data structures, and consequently very flexible containers for complex data. A concrete use would be to store a Height x Width x Channel array for different timepoints during an experiment. These may be the features output from an encoder neural network that processes a video.\n\n\n\nExamples of Tensor Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn analog time series has values that can vary continuously\n\n\n\nExamples of Analog Time Series Data\n\n\n\n\nVoltage traces from electrode recordings\n\n\nFluorescence intensity traces from Calcium imaging\n\n\n\n\n\n\n\n\n\nA digital event represents an ordered sequence of events, where each event is represented as a single instance in time. For instance, spike times from electrophysiology\n\n\n\nExamples of Event Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples of Interval Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedia is a sequence of images.\n\n\n\n\n\nExamples of Image Data\n\n\n\n\nImage sequence from two photon calcium imaging experiment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples of Video Data\n\n\n\n\nMP4 video from high speed scientific camera of behavior",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "developer/data_manager.html#data-types",
    "href": "developer/data_manager.html#data-types",
    "title": "Data Manager",
    "section": "",
    "text": "A Point represents a 2 dimensional (x, y) coordinate, which may vary in time. The PointData structure can hold multiple points per unit time.\n\n\n\nExamples of Point Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Line represents a 2 dimensional (x,y) collection of points, which may vary with time. The collection of points is ordered, meaning that each point is positioned relative to its neighbors. The LineData structure can hold multiple lines per unit time.\n\n\n\nExamples of Line Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA mask represents a 2 dimensional collection of points, which may vary with time. Compared to a Line, the points in a Mask are not ordered. These would correspond to something like a binary segmentation mask over an image. The MaskData structure can hold multiple masks per unit time.\n\n\n\nExamples of Mask Data\n\n\n\n\nBinary Semantic Segmentation Labels\n\n\n\n\n\n\n\n\n\n\n\n\nTensors are N-Dimensional data structures, and consequently very flexible containers for complex data. A concrete use would be to store a Height x Width x Channel array for different timepoints during an experiment. These may be the features output from an encoder neural network that processes a video.\n\n\n\nExamples of Tensor Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn analog time series has values that can vary continuously\n\n\n\nExamples of Analog Time Series Data\n\n\n\n\nVoltage traces from electrode recordings\n\n\nFluorescence intensity traces from Calcium imaging\n\n\n\n\n\n\n\n\n\nA digital event represents an ordered sequence of events, where each event is represented as a single instance in time. For instance, spike times from electrophysiology\n\n\n\nExamples of Event Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples of Interval Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedia is a sequence of images.\n\n\n\n\n\nExamples of Image Data\n\n\n\n\nImage sequence from two photon calcium imaging experiment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples of Video Data\n\n\n\n\nMP4 video from high speed scientific camera of behavior",
    "crumbs": [
      "Data Manager"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neuralyzer",
    "section": "",
    "text": "Neuralyzer is a cross-platform software package designed for analyzing and visualizing common forms of data generated during systems neuroscience experiments."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Neuralyzer",
    "section": "Installation",
    "text": "Installation\nThe software comes as compiled .exe, .dmg or tar files and can be immediately used on Windows, Mac and Linux - no install required !\nLatest Release: link\nOld Releases: link"
  },
  {
    "objectID": "index.html#supported-data-types",
    "href": "index.html#supported-data-types",
    "title": "Neuralyzer",
    "section": "Supported Data Types",
    "text": "Supported Data Types\nSome currently supported data types include:\n\nMultimedia - High speed video and collections of images\nArrays of simple 2D shapes - Points, Lines, and masks that vary with time.\nDigital time series data - Events, timestamps,\nAnalog time series data - Continuous movement variables"
  },
  {
    "objectID": "index.html#how-documentation-is-organized",
    "href": "index.html#how-documentation-is-organized",
    "title": "Neuralyzer",
    "section": "How Documentation is Organized",
    "text": "How Documentation is Organized\nThis documentation will follow the quadrants of the Diátaxis documentation authoring framework\n\nTutorials\nHow-to Guides\nConcepts\nReference Guides"
  },
  {
    "objectID": "labeling/mask.html",
    "href": "labeling/mask.html",
    "title": "Creating Mask Labels",
    "section": "",
    "text": "Creating mask labels is done through the GrabCut tool. Currently it can be opened through the Tongue Tracking widget."
  },
  {
    "objectID": "labeling/mask.html#grabcut-tool-operation",
    "href": "labeling/mask.html#grabcut-tool-operation",
    "title": "Creating Mask Labels",
    "section": "GrabCut Tool Operation",
    "text": "GrabCut Tool Operation\n\nSelecting ROI\nTo start creating a mask, left click and drag the mouse over the image to form a green rectangle as the region of interest. The objected intended to be masked should be completely within this rectangle. If a mistake is made the reset button returns the tool to the initial state.\nThe GrabCut algorithm works as such: the algorithm keeps track of the mask and for each iteration, attempts to refine it. In between iterations, the user can tweak the mask to provide feedback to the algorithm, which will be noted through subsequent iterations.\nAfter drawing an ROI the “Iterate Grabcut” button becomes functional. Using it completes one iteration of the GrabCut algorithm. The first usage of this button will show the initial mask created by the algorithm. Ideally the user should press this button throughout the editing process.\n\nOpacity of the displayed mask can be adjusted with the transparency slider\n\n\n\nUser Feedback\nIn between iterations the user has access to a drawing bush whose radius can be adjusted. It is used to paint feedback for the GrabCut algorithm, which it will take into account upon pressing the “Iterate Grabcut” button. The brush has four colors:\n\n“Definite Background”: Tells GrabCut the painted area is definitely not part of the mask.\n“Definite Foreground”: Tells GrabCut the painted area is definitely part of the mask.\n“Probable Background”: Suggests to GrabCut the painted area may not be part of the mask. GrabCut uses this information to create a better mask but may partially/fully disobey it.\n“Probable Foreground”: Suggests to GrabCut the painted area is likely part of the mask. GrabCut uses this information to create a better mask but may partially/fully disobey it.\n\n\n\nSaving and Exporting\nThe GrabCut tool may be exited at any time by pressing the “Exit” button. “Save and Exit” will exit the tool and save the mask into the main application and displayed in the media player. All created masks can be saved to disk using the “Save Drawn Masks” button in the Tongue Tracking widget."
  },
  {
    "objectID": "user_guide/behaviors/whisker.html",
    "href": "user_guide/behaviors/whisker.html",
    "title": "Whisker Tracking",
    "section": "",
    "text": "Load Whiskers\n\nSupported Whisker File Formats\n\n\n\n\n\n\nFile Format\nDescription\n\n\n\n\nJanelia\nBinary format output by janelia whisker tracker\n\n\nCSV\nEach row represents a 2d point (x,y) along the whisker. The points should be arranged from follicle to tip\n\n\nHDF5\n\n\n\n\n\n\nLoad Keypoints\n\n\nTrace Button\n\n\nLength Threshold\nWhisker segments below the length threshold will be discarded. Length threshold is in units of pixels\n\n\nWhisker Pad Selection\nThe whisker pad location in pixel coordinates. Candidate whiskers are ordered so that the base of the whisker is nearest the whisker pad. Whiskers with bases beyond some distance from the whisker pad can also be discarded.\n\n\nHead Orientation\nThe head orientation is the direction that the animal’s nose is pointing in the image. The head orientation is used to determine the identity of the whiskers in the frame (most posterior whisker is 0, next most posterior is 1, etc).\n\n\nNumber of Whiskers Selection\n\n\nExport Image and CSV Button\n\n\nFace Mask\nThe face mask corresponds to the part of the image belonging to the face. This can be used in several ways\n\nWhisker bases can be extended to always connect to the face mask. This eliminates jitter that can occur because of fur\nWhisker bases can be clipped to ensure that the whisker does not enter the face mask.\n\n\n\nJanelia Settings\n\n\nContact Detection",
    "crumbs": [
      "Behavioral Modules",
      "Whisker Tracking"
    ]
  },
  {
    "objectID": "user_guide/image_processing/bilateral_filter.html",
    "href": "user_guide/image_processing/bilateral_filter.html",
    "title": "Bilateral Filter",
    "section": "",
    "text": "https://docs.opencv.org/4.x/js_filtering_bilateralFilter.html\nhttps://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga9d7064d478c95d60003cf839430737ed\nhttps://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html",
    "crumbs": [
      "Image Processing",
      "Bilateral Filter"
    ]
  },
  {
    "objectID": "user_guide/image_processing/bilateral_filter.html#references",
    "href": "user_guide/image_processing/bilateral_filter.html#references",
    "title": "Bilateral Filter",
    "section": "",
    "text": "https://docs.opencv.org/4.x/js_filtering_bilateralFilter.html\nhttps://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga9d7064d478c95d60003cf839430737ed\nhttps://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html",
    "crumbs": [
      "Image Processing",
      "Bilateral Filter"
    ]
  },
  {
    "objectID": "user_guide/image_processing/contrast.html",
    "href": "user_guide/image_processing/contrast.html",
    "title": "Contrast and Brightness Adjustment",
    "section": "",
    "text": "These two filters are implemented with a single linear transform taking parameters “alpha” (\\(\\alpha\\)) and “beta” (\\(\\beta\\)). Pixel values are multiplied by \\(\\alpha\\) to adjust contrast, then have \\(\\beta\\) added to adjust brightness: \\[\ng(x) = \\alpha f(x) + \\beta\n\\] In effect the magnitude of \\(\\alpha\\) corresponds to the amount of contrast and the magnitude of \\(\\beta\\) corresponds to the amount of brightness.",
    "crumbs": [
      "Image Processing",
      "Contrast and Brightness Adjustment"
    ]
  }
]